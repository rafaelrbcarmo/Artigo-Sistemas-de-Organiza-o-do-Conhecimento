# -*- coding: utf-8 -*-
"""Cópia de grafos_do_conhecimento.ipynb correção.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fzV4CqhEJzp6jHdG4KFEbJSvEAbSS-xC
"""

!pip install requests pandas networkx matplotlib pyvis cdlib leidenalg

import os
import requests
import time
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network
from cdlib import algorithms
from IPython.display import HTML, display
from google.colab import userdata


# Configurações iniciais
tmdb_api_key = userdata.get('TMDB_API_KEY')
BASE_URL = "https://api.themoviedb.org/3"

ano_pesquisa = 2024
id_genero_documentario = 99
min_votos = 50
min_duracao = 60


def buscar_filmes(api_key, ano, genero_id, votos_minimos):
    """
    Busca filmes com base nos critérios definidos.
    """
    filmes_encontrados = []
    pagina = 1
    total_paginas = 1

    print("--- Buscando filmes de acordo com os critérios da pesquisa ---")
    print(f"Ano: {ano}, Gênero: Documentário, Votos: >{votos_minimos}")

    while pagina <= total_paginas:
        params = {
            'api_key': api_key,
            'language': 'pt-BR',
            'primary_release_year': ano,
            'with_genres': genero_id,
            'vote_count.gte': votos_minimos,
            'sort_by': 'popularity.desc',
            'page': pagina
        }

        try:
            response = requests.get(f"{BASE_URL}/discover/movie", params=params)
            response.raise_for_status()
            data = response.json()
            filmes_encontrados.extend(data.get('results', []))

            if pagina == 1:
                total_paginas = data.get('total_pages', 1)
                print(f"Encontrados {data.get('total_results', 0)} resultados em {total_paginas} página(s).")

            pagina += 1
            time.sleep(0.25)

        except requests.RequestException as e:
            print(f"Erro ao buscar página {pagina}: {e}")
            break

    return filmes_encontrados


def obter_detalhes_e_keywords(api_key, movie_id):
    """
    Coleta detalhes e palavras-chave de um filme específico.
    """
    try:
        detalhes_url = f"{BASE_URL}/movie/{movie_id}?api_key={api_key}&language=pt-BR"
        resposta_detalhes = requests.get(detalhes_url)
        resposta_detalhes.raise_for_status()
        dados_filme = resposta_detalhes.json()

        keywords_url = f"{BASE_URL}/movie/{movie_id}/keywords?api_key={api_key}"
        resposta_keywords = requests.get(keywords_url)
        resposta_keywords.raise_for_status()
        dados_keywords = resposta_keywords.json()

        return {
            'id': dados_filme['id'],
            'title': dados_filme['title'],
            'runtime': dados_filme.get('runtime', 0),
            'keywords': [kw['name'] for kw in dados_keywords.get('keywords', [])]
        }

    except requests.RequestException as e:
        print(f"Erro ao buscar detalhes para o filme ID {movie_id}: {e}")
        return None


def construir_grafos(filmes_info):
    """
    Constrói grafos com os filmes e suas palavras-chave.
    """
    print("\n--- Construindo os Grafos ---")
    grafo_principal = nx.Graph()
    grafo_keywords = nx.Graph()

    for filme_id, info in filmes_info.items():
        titulo, palavras = info['title'], info['keywords']
        grafo_principal.add_node(filme_id, type='filme', label=titulo, title=f"Filme: {titulo}", size=25, color='skyblue')

        for palavra in palavras:
            grafo_principal.add_node(palavra, type='keyword', label=palavra, title=f"Keyword: {palavra}", size=15)
            grafo_principal.add_edge(filme_id, palavra)

        for i, kw1 in enumerate(palavras):
            for j, kw2 in enumerate(palavras):
                if i < j:
                    if grafo_keywords.has_edge(kw1, kw2):
                        grafo_keywords[kw1][kw2]['weight'] += 1
                    else:
                        grafo_keywords.add_edge(kw1, kw2, weight=1)

    print(f"Grafo principal: {grafo_principal.number_of_nodes()} nós, {grafo_principal.number_of_edges()} arestas.")
    print(f"Grafo de keywords: {grafo_keywords.number_of_nodes()} nós, {grafo_keywords.number_of_edges()} arestas.")
    return grafo_principal, grafo_keywords


def detectar_comunidades_leiden(grafo_keywords):
    """
    Aplica o algoritmo de Leiden para identificar comunidades de palavras-chave.
    """
    print("\n--- Identificando comunidades com Leiden ---")

    if grafo_keywords.number_of_nodes() == 0:
        print("Grafo de keywords vazio. Pulando.")
        return []

    resultado = algorithms.leiden(grafo_keywords)
    comunidades = resultado.communities

    print(f"Detectadas {len(comunidades)} comunidades.")
    for i, grupo in enumerate(comunidades):
        amostra = grupo[:70]
        print(f"  Comunidade {i+1} ({len(grupo)} palavras): {', '.join(amostra)}{'...' if len(grupo) > 70 else ''}")
    return comunidades


def visualizar_com_pyvis(grafo, comunidades, nome_arquivo="grafo_documentarios_leiden.html"):
    """
    Cria um grafo interativo e salva como HTML.
    """
    print("\n--- Gerando visualização interativa com Pyvis ---")
    net = Network(notebook=True, height="800px", width="100%", bgcolor="#222222", font_color="white", cdn_resources='remote')

    keyword_to_color = {}
    try:
        cores = plt.colormaps.get_cmap('tab20').colors
    except AttributeError:
        cores = plt.cm.get_cmap('tab20', len(comunidades)).colors

    for i, comunidade in enumerate(comunidades):
        cor = '#%02x%02x%02x' % (
            int(cores[i % len(cores)][0]*255),
            int(cores[i % len(cores)][1]*255),
            int(cores[i % len(cores)][2]*255)
        )
        for palavra in comunidade:
            keyword_to_color[palavra] = cor

    for node_id, atributos in grafo.nodes(data=True):
        if atributos['type'] == 'keyword':
            atributos['color'] = keyword_to_color.get(node_id, 'grey')
        net.add_node(node_id, **atributos)

    for u, v in grafo.edges():
        net.add_edge(u, v, color="rgba(128, 128, 128, 0.3)")

    net.set_options("""
    var options = {
      "physics": {
        "forceAtlas2Based": {
          "gravitationalConstant": -50,
          "centralGravity": 0.01,
          "springLength": 100,
          "springConstant": 0.08
        },
        "minVelocity": 0.75,
        "solver": "forceAtlas2Based"
      },
      "interaction": {
        "hover": true,
        "tooltipDelay": 200
      }
    }
    """)

    net.show(nome_arquivo)
    print(f"Grafo salvo como '{nome_arquivo}'")
    display(HTML(nome_arquivo))  # ✅ Mantido como você pediu


# Execução principal
if __name__ == "__main__":
    filmes_iniciais = buscar_filmes(tmdb_api_key, ano_pesquisa, id_genero_documentario, min_votos)

    if not filmes_iniciais:
        print("\nNenhum filme encontrado. Encerrando.")
    else:
        print(f"\n--- Verificando detalhes e aplicando filtro de duração (> {min_duracao} min) ---")
        filmes_detalhados = []

        for resumo in filmes_iniciais:
            detalhes = obter_detalhes_e_keywords(tmdb_api_key, resumo['id'])
            if detalhes and detalhes.get('runtime', 0) > min_duracao:
                filmes_detalhados.append(detalhes)
                print(f"  - '{detalhes['title']}' [APROVADO] (Duração: {detalhes['runtime']} min)")
            else:
                status = f"(Duração: {detalhes.get('runtime', 0)} min)" if detalhes else "(Erro ao obter detalhes)"
                print(f"  - '{resumo['title']}' [REJEITADO] {status}")
            time.sleep(0.25)

        if not filmes_detalhados:
            print("\nNenhum filme passou no filtro de duração.")
        else:
            info_filmes = {
                filme['id']: {
                    'title': filme['title'],
                    'keywords': filme['keywords']
                } for filme in filmes_detalhados
            }

            grafo_principal, grafo_de_keywords = construir_grafos(info_filmes)
            comunidades = detectar_comunidades_leiden(grafo_de_keywords)
            visualizar_com_pyvis(grafo_principal, comunidades)